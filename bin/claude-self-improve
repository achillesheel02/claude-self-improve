#!/bin/bash
# Claude Self-Improvement System
# Analyzes Claude Code session performance data and updates persistent memory
# Usage: claude-self-improve [--interactive] [--bootstrap] [--dry-run] [--recall "query"] [--memory-dir DIR]

set +e

# Prevent concurrent runs (cron + manual overlap)
LOCKDIR="/tmp/claude-self-improve.lock"
if ! mkdir "$LOCKDIR" 2>/dev/null; then
    echo "Another claude-self-improve instance is running. Exiting."
    exit 1
fi
trap 'rmdir "$LOCKDIR" 2>/dev/null' EXIT

# ─── Configuration ───────────────────────────────────────────────────────────
DATA_DIR="${CLAUDE_IMPROVE_DATA_DIR:-$HOME/.local/share/claude-improve}"
PROMPTS_DIR="$DATA_DIR/prompts"
BACKUPS_DIR="$DATA_DIR/backups"
FACETS_DIR="$HOME/.claude/usage-data/facets"

# Auto-detect memory directory: check env var, then find active project memory
if [ -n "$CLAUDE_MEMORY_DIR" ]; then
    MEMORY_DIR="$CLAUDE_MEMORY_DIR"
else
    # Find the most recently modified MEMORY.md under Claude's project memory dirs
    MEMORY_DIR=$(find "$HOME/.claude/projects" -name "MEMORY.md" -maxdepth 3 2>/dev/null \
        | xargs ls -t 2>/dev/null \
        | head -1 \
        | xargs dirname 2>/dev/null)

    if [ -z "$MEMORY_DIR" ]; then
        echo "ERROR: Could not auto-detect memory directory."
        echo "Set CLAUDE_MEMORY_DIR or pass --memory-dir <path>"
        exit 1
    fi
fi

MEMORY_FILE="$MEMORY_DIR/MEMORY.md"
LAST_RUN="$DATA_DIR/last-run.json"
METRICS_FILE="$DATA_DIR/metrics.jsonl"
SUGGESTIONS_FILE="$DATA_DIR/claude-md-suggestions.md"
LOG_FILE="$DATA_DIR/run.log"
MAX_BACKUPS=10

# Cost caps
ANALYZE_BUDGET="0.50"
UPDATE_BUDGET="0.30"

# ─── Parse Arguments ─────────────────────────────────────────────────────────
INTERACTIVE=false
BOOTSTRAP=false
DRY_RUN=false
RECALL_QUERY=""

while [[ $# -gt 0 ]]; do
    case $1 in
        --interactive) INTERACTIVE=true; shift ;;
        --bootstrap)   BOOTSTRAP=true; shift ;;
        --dry-run)     DRY_RUN=true; shift ;;
        --recall)
            RECALL_QUERY="$2"
            shift 2
            ;;
        --memory-dir)
            MEMORY_DIR="$2"
            MEMORY_FILE="$MEMORY_DIR/MEMORY.md"
            shift 2
            ;;
        -h|--help)
            echo "Usage: claude-self-improve [OPTIONS]"
            echo ""
            echo "Analyzes Claude Code session data and updates persistent memory."
            echo ""
            echo "Options:"
            echo "  --interactive      Show proposed changes and ask for confirmation"
            echo "  --bootstrap        Process ALL existing facets (first run or reprocess)"
            echo "  --dry-run          Run analysis but don't update any files"
            echo "  --recall QUERY     Search past sessions by semantic similarity"
            echo "  --memory-dir DIR   Path to Claude Code memory directory"
            echo "                     (auto-detected if not set)"
            echo ""
            echo "Environment Variables:"
            echo "  CLAUDE_MEMORY_DIR       Override memory directory path"
            echo "  CLAUDE_IMPROVE_DATA_DIR Override data directory (default: ~/.local/share/claude-improve)"
            echo ""
            echo "Stages (analysis mode):"
            echo "  1. Collect  — Gathers session facets from ~/.claude/usage-data/facets/"
            echo "  2. Analyze  — Sends to headless Claude (Sonnet) for pattern extraction"
            echo "  3. Update   — Applies memory updates via headless Claude (Sonnet)"
            echo ""
            echo "Recall mode (--recall):"
            echo "  Searches session history using headless Claude (Haiku) for semantic matching."
            echo "  Example: claude-self-improve --recall \"monetization strategy\""
            echo ""
            exit 0
            ;;
        *) echo "Unknown option: $1"; exit 1 ;;
    esac
done

# ─── Logging ─────────────────────────────────────────────────────────────────
log() {
    local msg="[$(date '+%Y-%m-%d %H:%M:%S')] $1"
    echo "$msg"
    echo "$msg" >> "$LOG_FILE"
}

# ─── Preflight Checks ───────────────────────────────────────────────────────
if [ ! -d "$FACETS_DIR" ]; then
    log "ERROR: Facets directory not found at $FACETS_DIR"
    log "  Claude Code must be installed and have usage data."
    exit 1
fi

if [ ! -f "$MEMORY_FILE" ]; then
    log "ERROR: MEMORY.md not found at $MEMORY_FILE"
    log "  Create one or set CLAUDE_MEMORY_DIR to the correct path."
    exit 1
fi

if ! command -v claude &>/dev/null; then
    log "ERROR: claude CLI not found in PATH"
    log "  Install Claude Code: https://docs.anthropic.com/en/docs/claude-code"
    exit 1
fi

if ! command -v python3 &>/dev/null; then
    log "ERROR: python3 not found in PATH"
    exit 1
fi

if ! command -v jq &>/dev/null; then
    log "ERROR: jq not found in PATH"
    log "  Install: brew install jq (macOS) or apt install jq (Linux)"
    exit 1
fi

mkdir -p "$DATA_DIR" "$PROMPTS_DIR" "$BACKUPS_DIR"

# Ensure prompts exist (copy from repo if installed via install.sh)
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
REPO_PROMPTS="$SCRIPT_DIR/../prompts"
if [ -d "$REPO_PROMPTS" ]; then
    for f in "$REPO_PROMPTS"/*.md; do
        [ -f "$f" ] && cp -n "$f" "$PROMPTS_DIR/" 2>/dev/null
    done
fi

# ─── Recall Mode (early exit) ──────────────────────────────────────────────
if [ -n "$RECALL_QUERY" ]; then
    RECALL_BUDGET="0.15"
    RECALL_PROMPT_FILE="$PROMPTS_DIR/recall.md"

    if [ ! -f "$RECALL_PROMPT_FILE" ]; then
        log "ERROR: Recall prompt not found at $RECALL_PROMPT_FILE"
        log "  Run install.sh or copy prompts/ to $PROMPTS_DIR/"
        exit 1
    fi

    log "Session Recall: \"$RECALL_QUERY\""
    log "─────────────────────────────────────────────"

    # Build session index from facets + history.jsonl
    RECALL_INDEX="$DATA_DIR/recall-index.json"
    RECALL_INPUT="$DATA_DIR/recall-input.txt"

    python3 << 'PYEOF' > "$RECALL_INDEX"
import json, glob, os, sys
from collections import defaultdict

facets_dir = os.path.expanduser('~/.claude/usage-data/facets')
history_file = os.path.expanduser('~/.claude/history.jsonl')
claude_projects_dir = os.path.expanduser('~/.claude/projects')

# 1. Read all facets -> {session_id: {summary, goal}}
facet_data = {}
for f in sorted(glob.glob(os.path.join(facets_dir, '*.json')), key=os.path.getmtime):
    try:
        with open(f) as fh:
            d = json.load(fh)
            sid = d.get('session_id', '')
            if sid:
                facet_data[sid] = {
                    'summary': d.get('brief_summary', ''),
                    'goal': d.get('underlying_goal', ''),
                }
    except:
        pass

# 2. Read history.jsonl -> {sessionId: {project, messages, timestamp}}
history_data = defaultdict(lambda: {'project': '', 'messages': [], 'timestamp': ''})
if os.path.exists(history_file):
    with open(history_file) as hf:
        for line in hf:
            try:
                entry = json.loads(line.strip())
                sid = entry.get('sessionId', '')
                if not sid:
                    continue
                h = history_data[sid]
                proj = entry.get('project', '')
                if proj:
                    h['project'] = proj
                ts = entry.get('timestamp', '')
                if ts:
                    h['timestamp'] = ts
                # Collect all user messages (prioritized later)
                display = entry.get('display', '')
                if display:
                    h['messages'].append(display)
            except:
                pass

# 3. Merge into session index
sessions = []
all_sids = set(list(facet_data.keys()) + list(history_data.keys()))

for sid in all_sids:
    facet = facet_data.get(sid, {})
    hist = history_data.get(sid, {})

    summary = facet.get('summary', '')
    goal = facet.get('goal', '')
    project = hist.get('project', '') if hist else ''
    timestamp = hist.get('timestamp', '') if hist else ''

    # Priority sampling: longer messages = more informative, cap at 800 chars total
    raw_msgs = hist.get('messages', []) if hist else []
    sorted_msgs = sorted(raw_msgs, key=len, reverse=True)
    messages = []
    total_chars = 0
    for msg in sorted_msgs:
        snippet = msg[:120]
        if total_chars + len(snippet) > 800:
            break
        messages.append(snippet)
        total_chars += len(snippet)

    # Skip sessions with no useful metadata
    if not summary and not goal and not messages:
        continue

    # Extract date from timestamp (may be ISO string or epoch millis)
    date = ''
    if timestamp:
        if isinstance(timestamp, (int, float)):
            from datetime import datetime, timezone
            date = datetime.fromtimestamp(timestamp / 1000, tz=timezone.utc).strftime('%Y-%m-%d')
        elif isinstance(timestamp, str):
            date = timestamp[:10]

    # Find transcript path
    transcript = ''
    if project:
        encoded = project.replace('/', '-').lstrip('-')
        encoded_us = project.replace('/', '-').replace('_', '-').lstrip('-')
        for enc in [encoded, encoded_us]:
            candidate = os.path.join(claude_projects_dir, f'-{enc}', f'{sid}.jsonl')
            if os.path.exists(candidate):
                transcript = candidate
                break

    sessions.append({
        'session_id': sid,
        'project': project,
        'date': date,
        'summary': summary,
        'goal': goal,
        'messages': messages,
        'transcript': transcript,
    })

# Sort by date descending (most recent first)
sessions.sort(key=lambda s: s.get('date', ''), reverse=True)

json.dump(sessions, sys.stdout)
PYEOF

    SESSION_COUNT=$(python3 -c "import json; print(len(json.load(open('$RECALL_INDEX'))))" 2>/dev/null || echo "0")
    log "  Indexed $SESSION_COUNT sessions from facets + history"

    # Build recall input for Haiku
    {
        echo "Find sessions matching this query: $RECALL_QUERY"
        echo ""
        echo "=== SESSION INDEX ==="
        cat "$RECALL_INDEX"
    } > "$RECALL_INPUT"

    log "  Querying headless Claude (Haiku)..."

    # Call headless Claude with Haiku
    RECALL_OUTPUT="$DATA_DIR/recall-output.json"
    cat "$RECALL_INPUT" | claude -p - \
        --append-system-prompt "$(cat "$RECALL_PROMPT_FILE")" \
        --output-format json \
        --model haiku \
        --tools "" \
        --max-budget-usd "$RECALL_BUDGET" \
        > "$RECALL_OUTPUT" 2>"$DATA_DIR/recall-stderr.log"

    RECALL_EXIT=$?
    if [ $RECALL_EXIT -ne 0 ]; then
        log "ERROR: Recall query failed (exit code $RECALL_EXIT)"
        log "  Check $DATA_DIR/recall-stderr.log"
        exit 1
    fi

    # Extract result from Claude's JSON wrapper and save as clean text
    RECALL_RESULT_FILE="$DATA_DIR/recall-result.txt"
    # Check for budget exceeded or error
    RECALL_SUBTYPE=$(jq -r '.subtype // ""' "$RECALL_OUTPUT" 2>/dev/null)
    if [ "$RECALL_SUBTYPE" = "error_max_budget_usd" ]; then
        log "ERROR: Recall query exceeded budget cap (\$$RECALL_BUDGET)"
        log "  Session index may be too large. Try reducing facets or raising budget."
        exit 1
    fi
    jq -r '.result // .' "$RECALL_OUTPUT" > "$RECALL_RESULT_FILE" 2>/dev/null

    # Parse and display results
    RECALL_DISPLAY_SCRIPT="$DATA_DIR/recall-display.py"
    cat > "$RECALL_DISPLAY_SCRIPT" << 'PYEOF'
import json, sys, os, re

result_file = sys.argv[1]
index_file = sys.argv[2]

with open(result_file) as f:
    raw = f.read().strip()

# Strip markdown fences if present
raw = re.sub(r'^```json\s*', '', raw)
raw = re.sub(r'\s*```$', '', raw)

try:
    data = json.loads(raw)
except json.JSONDecodeError:
    # Try to find JSON in text
    match = re.search(r'\{[\s\S]*\}', raw)
    if match:
        data = json.loads(match.group())
    else:
        print('ERROR: Could not parse recall results as JSON')
        print(f'Raw: {raw[:500]}')
        sys.exit(1)

matches = data.get('matches', [])

# Load session index to look up transcript paths
transcript_map = {}
if os.path.exists(index_file):
    with open(index_file) as f:
        for s in json.load(f):
            transcript_map[s['session_id']] = s.get('transcript', '')

if not matches:
    print('  No matching sessions found.')
    sys.exit(0)

for i, m in enumerate(matches, 1):
    sid = m.get('session_id', '?')
    project = m.get('project', '?')
    date = m.get('date', '?')
    relevance = m.get('relevance', '?').upper()
    reason = m.get('reason', '')
    summary = m.get('summary', '')
    transcript = transcript_map.get(sid, '')

    print(f'  {i}. [{relevance}] {date} — {project}')
    print(f'     {reason}')
    if summary:
        print(f'     Summary: {summary}')
    if transcript:
        print(f'     Transcript: {transcript}')
    else:
        print(f'     Session ID: {sid}')
    print()

print(f'  Found {len(matches)} matching session(s).')
PYEOF

    python3 "$RECALL_DISPLAY_SCRIPT" "$RECALL_RESULT_FILE" "$RECALL_INDEX" 2>&1 | while IFS= read -r line; do log "$line"; done

    # Clean up
    rm -f "$RECALL_INDEX" "$RECALL_INPUT"
    exit 0
fi

log "═══════════════════════════════════════════════════"
log "  CLAUDE SELF-IMPROVEMENT RUN"
log "  Mode: $([ "$BOOTSTRAP" = true ] && echo "BOOTSTRAP" || echo "incremental")"
log "  Interactive: $INTERACTIVE | Dry-run: $DRY_RUN"
log "  Memory: $MEMORY_FILE"
log "═══════════════════════════════════════════════════"

# ─── Stage 1: Collect ────────────────────────────────────────────────────────
log "Stage 1: Collecting session data..."

# Count total facets
TOTAL_FACETS=$(ls "$FACETS_DIR"/*.json 2>/dev/null | wc -l | tr -d ' ')
log "  Total facets on disk: $TOTAL_FACETS"

# Determine which facets to process
LAST_COUNT=0
if [ -f "$LAST_RUN" ] && [ "$BOOTSTRAP" = false ]; then
    LAST_COUNT=$(jq -r '.facets_processed // 0' "$LAST_RUN" 2>/dev/null || echo "0")
fi

if [ "$BOOTSTRAP" = true ]; then
    PROCESS_COUNT=$TOTAL_FACETS
    log "  Bootstrap mode: processing ALL $TOTAL_FACETS facets"
else
    PROCESS_COUNT=$((TOTAL_FACETS - LAST_COUNT))
    log "  Previously processed: $LAST_COUNT"
    log "  New facets to process: $PROCESS_COUNT"
fi

# Check if there's new data
if [ "$PROCESS_COUNT" -le 0 ] && [ "$BOOTSTRAP" = false ]; then
    log "No new sessions to analyze. Run with --bootstrap to reprocess all."
    # macOS notification (silently skip on Linux)
    osascript -e 'display notification "No new sessions to analyze" with title "Claude Self-Improve"' 2>/dev/null
    exit 0
fi

# Read current MEMORY.md line count
MEMORY_LINES=$(wc -l < "$MEMORY_FILE" | tr -d ' ')

# Assemble payload using Python (avoids shell quoting issues with JSON)
PAYLOAD_FILE="$DATA_DIR/payload.json"
FACETS_TMP="$DATA_DIR/facets-tmp.json"
METRICS_TMP="$DATA_DIR/metrics-tmp.json"

# Write facets to temp file
if [ "$BOOTSTRAP" = true ]; then
    SLICE_COUNT=0  # 0 means all
else
    SLICE_COUNT=$PROCESS_COUNT
fi

python3 -c "
import json, glob, os, sys

facets_dir = '$FACETS_DIR'
files = sorted(glob.glob(os.path.join(facets_dir, '*.json')), key=os.path.getmtime)
slice_count = $SLICE_COUNT
if slice_count > 0:
    files = files[-slice_count:]

facets = []
for f in files:
    with open(f) as fh:
        facets.append(json.load(fh))

# Build session -> project lookup from history.jsonl
session_projects = {}
history_file = os.path.expanduser('~/.claude/history.jsonl')
if os.path.exists(history_file):
    with open(history_file) as hf:
        for line in hf:
            try:
                entry = json.loads(line.strip())
                sid = entry.get('sessionId', '')
                proj = entry.get('project', '')
                if sid and proj:
                    session_projects[sid] = proj
            except:
                pass

# Annotate each facet with its project path
matched = 0
for facet in facets:
    sid = facet.get('session_id', '')
    proj = session_projects.get(sid, 'unknown')
    facet['project_path'] = proj
    if proj != 'unknown':
        matched += 1

with open('$FACETS_TMP', 'w') as out:
    json.dump(facets, out)

print(f'Collected {len(facets)} facets ({matched}/{len(facets)} matched to projects)')
" 2>&1 | while read -r line; do log "  $line"; done

# Write previous metrics to temp file
if [ -f "$METRICS_FILE" ]; then
    tail -12 "$METRICS_FILE" | python3 -c "
import sys, json
lines = [l.strip() for l in sys.stdin if l.strip()]
metrics = []
for l in lines:
    try:
        metrics.append(json.loads(l))
    except:
        pass
with open('$METRICS_TMP', 'w') as out:
    json.dump(metrics, out)
" 2>/dev/null
fi
[ -f "$METRICS_TMP" ] || echo "[]" > "$METRICS_TMP"

# Assemble final payload from temp files (safe from shell quoting)
python3 -c "
import json

with open('$FACETS_TMP') as f:
    facets = json.load(f)
with open('$METRICS_TMP') as f:
    prev_metrics = json.load(f)
with open('$MEMORY_FILE') as f:
    memory_content = f.read()

payload = {
    'facets': facets,
    'current_memory': memory_content,
    'memory_line_count': $MEMORY_LINES,
    'previous_metrics': prev_metrics,
    'is_bootstrap': $([ "$BOOTSTRAP" = true ] && echo "True" || echo "False")
}

with open('$PAYLOAD_FILE', 'w') as out:
    json.dump(payload, out, indent=2)

print(f'Payload assembled: {len(facets)} facets, {$MEMORY_LINES} memory lines')
" 2>&1 | while read -r line; do log "  $line"; done

# Clean up temp files
rm -f "$FACETS_TMP" "$METRICS_TMP"

if [ ! -f "$PAYLOAD_FILE" ]; then
    log "ERROR: Failed to assemble payload"
    exit 1
fi

log "Stage 1 complete."

# ─── Stage 2: Analyze ────────────────────────────────────────────────────────
log ""
log "Stage 2: Analyzing session data with headless Claude..."

ANALYSIS_FILE="$DATA_DIR/analysis.json"
ANALYZE_PROMPT_FILE="$PROMPTS_DIR/analyze.md"

if [ ! -f "$ANALYZE_PROMPT_FILE" ]; then
    log "ERROR: Analysis prompt not found at $ANALYZE_PROMPT_FILE"
    log "  Run install.sh or copy prompts/ to $PROMPTS_DIR/"
    exit 1
fi

# Build a prompt file that includes instructions + payload
PROMPT_FILE="$DATA_DIR/analyze-prompt.txt"
{
    echo "Analyze this session performance data and return JSON only (no markdown fences)."
    echo ""
    echo "=== PAYLOAD ==="
    cat "$PAYLOAD_FILE"
} > "$PROMPT_FILE"

# Pipe prompt via stdin to avoid shell argument length limits
cat "$PROMPT_FILE" | claude -p - \
    --append-system-prompt "$(cat "$ANALYZE_PROMPT_FILE")" \
    --allowedTools "Read" "Glob" "Grep" \
    --output-format json \
    --model sonnet \
    --max-budget-usd "$ANALYZE_BUDGET" \
    > "$ANALYSIS_FILE" 2>"$DATA_DIR/analyze-stderr.log"

ANALYZE_EXIT=$?

if [ $ANALYZE_EXIT -ne 0 ]; then
    log "ERROR: Analysis stage failed (exit code $ANALYZE_EXIT)"
    log "  Check $DATA_DIR/analyze-stderr.log for details"
    exit 1
fi

# Extract the result text from the JSON output
# claude --output-format json wraps in {"type":"result","subtype":"success","result":"..."}
ANALYSIS_RESULT=$(jq -r '.result // .' "$ANALYSIS_FILE" 2>/dev/null)

# Parse analysis result — strip markdown fences, extract JSON
PARSE_SCRIPT="$DATA_DIR/parse-analysis.py"
cat > "$PARSE_SCRIPT" << 'PYEOF'
import sys, json, re

text = sys.stdin.read().strip()

# Strip markdown code fences if present
text = re.sub(r'^```json\s*', '', text)
text = re.sub(r'\s*```$', '', text)

# Try to parse directly
try:
    data = json.loads(text)
    print(json.dumps(data, indent=2))
    sys.exit(0)
except json.JSONDecodeError:
    pass

# Try to find JSON object in the text
match = re.search(r'\{[\s\S]*\}', text)
if match:
    try:
        data = json.loads(match.group())
        print(json.dumps(data, indent=2))
        sys.exit(0)
    except json.JSONDecodeError:
        pass

# Fallback: output raw text
print(text)
sys.exit(1)
PYEOF

CLEAN_ANALYSIS=$(echo "$ANALYSIS_RESULT" | python3 "$PARSE_SCRIPT" 2>/dev/null)

if [ $? -ne 0 ]; then
    log "ERROR: Could not parse analysis as JSON."
    log "  Raw output saved to: $ANALYSIS_FILE"
    log "  Fix: re-run or check $DATA_DIR/analyze-stderr.log"
    exit 1
fi

echo "$CLEAN_ANALYSIS" > "$ANALYSIS_FILE"

# Display summary
SESSIONS=$(echo "$CLEAN_ANALYSIS" | jq -r '.sessions_analyzed // "?"' 2>/dev/null)
FRICTION_RATE=$(echo "$CLEAN_ANALYSIS" | jq -r '.friction_summary.friction_rate // "?"' 2>/dev/null)
TREND=$(echo "$CLEAN_ANALYSIS" | jq -r '.trend // "unknown"' 2>/dev/null)
UPDATE_COUNT=$(echo "$CLEAN_ANALYSIS" | jq -r '.memory_updates | length // 0' 2>/dev/null)
SUGGESTION_COUNT=$(echo "$CLEAN_ANALYSIS" | jq -r '.claude_md_suggestions | length // 0' 2>/dev/null)

log ""
log "  Analysis Results:"
log "  ─────────────────────────────"
log "  Sessions analyzed:  $SESSIONS"
log "  Friction rate:      $FRICTION_RATE"
log "  Trend:              $TREND"
log "  Memory updates:     $UPDATE_COUNT proposed"
log "  CLAUDE.md suggests: $SUGGESTION_COUNT"
log ""

# Show details of each proposed memory update
if [ "$UPDATE_COUNT" -gt 0 ] 2>/dev/null; then
    log "  Proposed Memory Updates:"
    log "  ─────────────────────────────"
    echo "$CLEAN_ANALYSIS" | jq -r '.memory_updates[]? | "  [\(.operation)] → \(.section // .filename)\n    \(.content | split("\n")[0])\n    Reason: \(.reason)\n"' 2>/dev/null | while IFS= read -r line; do
        log "$line"
    done
fi

# ─── Effectiveness Report ───────────────────────────────────────────────────
HAS_BASELINE=$(echo "$CLEAN_ANALYSIS" | jq -r '.effectiveness.has_baseline // false' 2>/dev/null)
if [ "$HAS_BASELINE" = "true" ]; then
    EFF_VERDICT=$(echo "$CLEAN_ANALYSIS" | jq -r '.effectiveness.overall_verdict // "unknown"' 2>/dev/null)
    EFF_BASELINE_FR=$(echo "$CLEAN_ANALYSIS" | jq -r '.effectiveness.baseline_friction_rate // "?"' 2>/dev/null)
    EFF_CURRENT_FR=$(echo "$CLEAN_ANALYSIS" | jq -r '.effectiveness.current_friction_rate // "?"' 2>/dev/null)
    EFF_FR_DELTA=$(echo "$CLEAN_ANALYSIS" | jq -r '.effectiveness.friction_rate_delta // "?"' 2>/dev/null)
    EFF_SAT_DELTA=$(echo "$CLEAN_ANALYSIS" | jq -r '.effectiveness.satisfaction_delta // "?"' 2>/dev/null)
    EFF_OUT_DELTA=$(echo "$CLEAN_ANALYSIS" | jq -r '.effectiveness.outcome_delta // "?"' 2>/dev/null)

    log ""
    log "  Effectiveness Report:"
    log "  ─────────────────────────────"
    log "  Overall verdict:    $EFF_VERDICT"
    log "  Friction rate:      $EFF_BASELINE_FR → $EFF_CURRENT_FR (delta: $EFF_FR_DELTA)"
    log "  Satisfaction delta: $EFF_SAT_DELTA"
    log "  Outcome delta:      $EFF_OUT_DELTA"

    # Per-type changes
    TYPE_CHANGES=$(echo "$CLEAN_ANALYSIS" | jq -r '.effectiveness.per_type_changes[]? | "    \(.type): \(.baseline_count) → \(.current_count) (\(.verdict))"' 2>/dev/null)
    if [ -n "$TYPE_CHANGES" ]; then
        log "  Per-type friction changes:"
        echo "$TYPE_CHANGES" | while IFS= read -r line; do
            log "$line"
        done
    fi

    # Likely effective updates
    EFFECTIVE_UPDATES=$(echo "$CLEAN_ANALYSIS" | jq -r '.effectiveness.likely_effective_updates[]?' 2>/dev/null)
    if [ -n "$EFFECTIVE_UPDATES" ]; then
        log "  Likely effective memory updates:"
        echo "$EFFECTIVE_UPDATES" | while IFS= read -r line; do
            log "    ✓ $line"
        done
    fi
    log ""
else
    BASELINE_RUNS=$(echo "$CLEAN_ANALYSIS" | jq -r '.effectiveness.baseline_runs // 0' 2>/dev/null)
    log ""
    log "  Effectiveness Report: insufficient data (need 2+ previous runs, have $BASELINE_RUNS)"
    log "  This run establishes baseline metrics for future comparison."
    log ""
fi

# Save CLAUDE.md suggestions (never auto-applied)
if [ "$SUGGESTION_COUNT" -gt 0 ] 2>/dev/null; then
    echo "# CLAUDE.md Suggestions ($(date '+%Y-%m-%d'))" > "$SUGGESTIONS_FILE"
    echo "" >> "$SUGGESTIONS_FILE"
    echo "These suggestions are based on friction patterns seen in 3+ sessions." >> "$SUGGESTIONS_FILE"
    echo "Review and apply manually if appropriate." >> "$SUGGESTIONS_FILE"
    echo "" >> "$SUGGESTIONS_FILE"
    echo "$CLEAN_ANALYSIS" | jq -r '.claude_md_suggestions[]? | "## \(.suggestion)\n- Evidence: \(.evidence)\n- Sessions seen: \(.sessions_seen)\n"' >> "$SUGGESTIONS_FILE" 2>/dev/null
    log "  CLAUDE.md suggestions saved to: $SUGGESTIONS_FILE"
fi

# Save metrics to time-series file
METRICS_ENTRY=$(echo "$CLEAN_ANALYSIS" | jq -c '.metrics // {}' 2>/dev/null)
if [ "$METRICS_ENTRY" != "{}" ] && [ -n "$METRICS_ENTRY" ]; then
    echo "$METRICS_ENTRY" >> "$METRICS_FILE"
    log "  Metrics appended to: $METRICS_FILE"
fi

log "Stage 2 complete."

# ─── Interactive Confirmation ────────────────────────────────────────────────
if [ "$INTERACTIVE" = true ] && [ "$UPDATE_COUNT" -gt 0 ] 2>/dev/null; then
    echo ""
    echo "═══════════════════════════════════════════════════"
    echo "  PROPOSED MEMORY UPDATES"
    echo "═══════════════════════════════════════════════════"
    echo ""
    echo "$CLEAN_ANALYSIS" | jq -r '.memory_updates[]? | "  [\(.operation)] \(.section // .filename):\n    \(.content | split("\n")[0])\n    Reason: \(.reason)\n"' 2>/dev/null
    echo ""
    read -p "Apply these updates? (y/n) " -n 1 -r
    echo ""
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        log "Updates cancelled by user."
        exit 0
    fi
fi

# ─── Dry-run Exit ────────────────────────────────────────────────────────────
if [ "$DRY_RUN" = true ]; then
    log "Dry-run mode: skipping Stage 3 (memory update)."

    # Show thread routing preview even in dry-run
    THREAD_COUNT=$(echo "$CLEAN_ANALYSIS" | jq -r '.active_threads | length // 0' 2>/dev/null)
    if [ "$THREAD_COUNT" -gt 0 ] 2>/dev/null; then
        log ""
        log "Stage 3.5: Thread routing (dry-run preview)..."
        echo "$CLEAN_ANALYSIS" | python3 -c "
import json, sys, os

analysis = json.load(sys.stdin)
threads = analysis.get('active_threads', [])
claude_projects_dir = os.path.expanduser('~/.claude/projects')

# Group by project
project_threads = {}
untagged = []
for t in threads:
    projects = t.get('projects', [])
    if not projects:
        untagged.append(t)
        continue
    for proj in projects:
        if proj and proj != 'unknown':
            project_threads.setdefault(proj, []).append(t)
        else:
            untagged.append(t)

print('  Thread Routing (dry-run):')
print('  ─────────────────────────────')
for proj_path, pt in project_threads.items():
    encoded = proj_path.replace('/', '-').replace('_', '-').lstrip('-')
    candidate = os.path.join(claude_projects_dir, f'-{encoded}', 'memory', 'MEMORY.md')
    exists = os.path.exists(candidate)
    if not exists:
        encoded2 = proj_path.replace('/', '-').lstrip('-')
        candidate = os.path.join(claude_projects_dir, f'-{encoded2}', 'memory', 'MEMORY.md')
        exists = os.path.exists(candidate)
    status = f'{len(pt)} thread(s) would be written' if exists else 'skipped — no MEMORY.md'
    print(f'  {proj_path} ({status})')
    for t in pt:
        print(f\"    {t['thread']} ({t.get('sessions', '?')} sessions)\")
if untagged:
    print(f'  (untagged) {len(untagged)} thread(s) — no project_path from analysis')
    for t in untagged:
        print(f\"    {t['thread']} ({t.get('sessions', '?')} sessions)\")
" 2>/dev/null | while IFS= read -r line; do log "$line"; done
    fi

    log ""
    log "Analysis saved to: $ANALYSIS_FILE"
    exit 0
fi

# ─── Stage 3: Update Memory ─────────────────────────────────────────────────
if [ "$UPDATE_COUNT" -le 0 ] 2>/dev/null; then
    log "No memory updates to apply. Skipping Stage 3."
else
    log ""
    log "Stage 3: Updating memory files..."

    # Backup current MEMORY.md
    BACKUP_FILE="$BACKUPS_DIR/MEMORY_$(date '+%Y%m%d_%H%M%S').md"
    cp "$MEMORY_FILE" "$BACKUP_FILE"
    log "  Backup saved to: $BACKUP_FILE"

    # Prune old backups (keep latest MAX_BACKUPS)
    BACKUP_COUNT=$(ls "$BACKUPS_DIR"/MEMORY_*.md 2>/dev/null | wc -l | tr -d ' ')
    if [ "$BACKUP_COUNT" -gt "$MAX_BACKUPS" ]; then
        ls -t "$BACKUPS_DIR"/MEMORY_*.md | tail -n +$((MAX_BACKUPS + 1)) | xargs rm -f
        log "  Pruned old backups (kept latest $MAX_BACKUPS)"
    fi

    # Also backup any topic files that will be modified
    for f in "$MEMORY_DIR"/*.md; do
        if [ -f "$f" ] && [ "$(basename "$f")" != "MEMORY.md" ]; then
            cp "$f" "$BACKUPS_DIR/$(basename "$f" .md)_$(date '+%Y%m%d_%H%M%S').md"
        fi
    done

    # Build update prompt file
    UPDATE_PROMPT_FILE="$PROMPTS_DIR/update-memory.md"

    if [ ! -f "$UPDATE_PROMPT_FILE" ]; then
        log "ERROR: Update prompt not found at $UPDATE_PROMPT_FILE"
        exit 1
    fi

    UPDATE_INPUT_FILE="$DATA_DIR/update-input.txt"
    {
        echo "Apply these memory updates to the files in $MEMORY_DIR/:"
        echo ""
        jq '{memory_updates, metrics}' "$ANALYSIS_FILE" 2>/dev/null
    } > "$UPDATE_INPUT_FILE"

    cat "$UPDATE_INPUT_FILE" | claude -p - \
        --append-system-prompt "$(cat "$UPDATE_PROMPT_FILE")" \
        --allowedTools "Read" "Edit" "Write" \
        --model sonnet \
        --max-budget-usd "$UPDATE_BUDGET" \
        > "$DATA_DIR/update-output.json" 2>"$DATA_DIR/update-stderr.log"

    UPDATE_EXIT=$?

    if [ $UPDATE_EXIT -ne 0 ]; then
        log "WARNING: Update stage had issues (exit code $UPDATE_EXIT)"
        log "  Check $DATA_DIR/update-stderr.log for details"
        log "  MEMORY.md backup available at: $BACKUP_FILE"
    else
        # Verify MEMORY.md line count
        NEW_LINES=$(wc -l < "$MEMORY_FILE" | tr -d ' ')
        log "  MEMORY.md updated: $MEMORY_LINES → $NEW_LINES lines"

        if [ "$NEW_LINES" -gt 180 ]; then
            log "  WARNING: MEMORY.md exceeds 180 line budget ($NEW_LINES lines)"
            log "  Consider running again or manually moving content to topic files"
        fi

        # Show actual diff of what changed
        DIFF_OUTPUT=$(diff "$BACKUP_FILE" "$MEMORY_FILE" 2>/dev/null)
        if [ -n "$DIFF_OUTPUT" ]; then
            log ""
            log "  Changes Applied to MEMORY.md:"
            log "  ─────────────────────────────"
            echo "$DIFF_OUTPUT" | while IFS= read -r line; do
                log "  $line"
            done
        fi

        # Show any new topic files created
        for f in "$MEMORY_DIR"/*.md; do
            fname=$(basename "$f")
            if [ "$fname" != "MEMORY.md" ] && [ ! -f "$BACKUPS_DIR/${fname%.md}_"*.md ] 2>/dev/null; then
                log ""
                log "  New topic file created: $fname"
            fi
        done
    fi

    log ""
    log "Stage 3 complete."
fi

# ─── Stage 3.5: Route Threads to Per-Project MEMORY.md ──────────────────────
THREAD_COUNT=$(echo "$CLEAN_ANALYSIS" | jq -r '.active_threads | length // 0' 2>/dev/null)

if [ "$THREAD_COUNT" -gt 0 ] 2>/dev/null && [ "$DRY_RUN" = false ]; then
    log ""
    log "Stage 3.5: Routing threads to per-project MEMORY.md files..."

    THREAD_ROUTING=$(ANALYSIS_FILE="$ANALYSIS_FILE" python3 << 'PYEOF'
import json, os, re

analysis_file = os.environ.get('ANALYSIS_FILE', '')
with open(analysis_file) as f:
    analysis = json.load(f)

threads = analysis.get('active_threads', [])
if not threads:
    print(json.dumps({"routed": [], "skipped": []}))
    exit(0)

# Group threads by project
project_threads = {}  # {project_path: [thread, ...]}
for t in threads:
    projects = t.get('projects', [])
    for proj in projects:
        if proj and proj != 'unknown':
            project_threads.setdefault(proj, []).append(t)

# For each project, check if MEMORY.md exists and write threads
claude_projects_dir = os.path.expanduser('~/.claude/projects')
routed = []
skipped = []

for proj_path, proj_threads in project_threads.items():
    # Encode project path to Claude's directory naming convention
    # /Users/bachillah/Documents/living_goods/Ona -> -Users-bachillah-Documents-living-goods-Ona
    # Replace path separators with hyphens, handle underscores->hyphens
    encoded = proj_path.replace('/', '-').lstrip('-')
    # Claude Code also replaces underscores with hyphens in directory names
    encoded_underscore = proj_path.replace('/', '-').replace('_', '-').lstrip('-')

    # Try both encodings (with and without underscore replacement)
    memory_path = None
    for enc in [encoded, encoded_underscore]:
        candidate = os.path.join(claude_projects_dir, f'-{enc}', 'memory', 'MEMORY.md')
        if os.path.exists(candidate):
            memory_path = candidate
            break

    if not memory_path:
        skipped.append({
            "project": proj_path,
            "threads": [t['thread'] for t in proj_threads],
            "reason": "no MEMORY.md"
        })
        continue

    # Build the ## Active Threads section
    lines = ["## Active Threads"]
    for t in proj_threads:
        status = t.get('status', 'active')
        sessions = t.get('sessions', 0)
        last_seen = t.get('last_seen', '?')
        summary = t.get('summary', '')
        lines.append(f"- **{t['thread']}** ({sessions} sessions, last: {last_seen}) — {status}")
        if summary:
            lines.append(f"  {summary}")
    thread_section = '\n'.join(lines) + '\n'

    # Read existing MEMORY.md
    with open(memory_path) as f:
        content = f.read()

    # Replace existing ## Active Threads section, or append before last section
    pattern = r'## Active Threads\n(?:(?!## ).+\n)*'
    if re.search(pattern, content):
        # Replace existing section
        new_content = re.sub(pattern, thread_section + '\n', content)
    else:
        # Append before the last ## section (or at end if no sections)
        sections = list(re.finditer(r'^## ', content, re.MULTILINE))
        if len(sections) >= 2:
            # Insert before the last section
            insert_pos = sections[-1].start()
            new_content = content[:insert_pos] + thread_section + '\n' + content[insert_pos:]
        else:
            # Append at end
            new_content = content.rstrip() + '\n\n' + thread_section

    # Check line count before writing
    new_line_count = new_content.count('\n')
    if new_line_count > 200:
        skipped.append({
            "project": proj_path,
            "threads": [t['thread'] for t in proj_threads],
            "reason": f"would exceed 200 lines ({new_line_count})"
        })
        continue

    with open(memory_path, 'w') as f:
        f.write(new_content)

    routed.append({
        "project": proj_path,
        "memory_path": memory_path,
        "threads": [{"thread": t['thread'], "sessions": t.get('sessions', 0)} for t in proj_threads]
    })

print(json.dumps({"routed": routed, "skipped": skipped}))
PYEOF
    )

    # Display thread routing results
    log ""
    log "  Thread Routing:"
    log "  ─────────────────────────────"
    echo "$THREAD_ROUTING" | python3 -c "
import sys, json
data = json.load(sys.stdin)
for r in data.get('routed', []):
    proj = r['project']
    count = len(r['threads'])
    print(f'  {proj} ({count} thread(s) written)')
    for t in r['threads']:
        print(f\"    {t['thread']} ({t['sessions']} sessions)\")
for s in data.get('skipped', []):
    proj = s['project']
    reason = s['reason']
    print(f'  {proj} (skipped — {reason})')
    for t in s.get('threads', []):
        print(f'    {t}')
" 2>/dev/null | while IFS= read -r line; do log "$line"; done

    log ""
    log "Stage 3.5 complete."
elif [ "$THREAD_COUNT" -gt 0 ] 2>/dev/null && [ "$DRY_RUN" = true ]; then
    log ""
    log "Stage 3.5: Thread routing (dry-run, not writing)..."

    # Still show what WOULD be routed
    echo "$CLEAN_ANALYSIS" | python3 -c "
import json, sys, os, re

analysis = json.load(sys.stdin)
threads = analysis.get('active_threads', [])
claude_projects_dir = os.path.expanduser('~/.claude/projects')

# Group by project
project_threads = {}
for t in threads:
    for proj in t.get('projects', []):
        if proj and proj != 'unknown':
            project_threads.setdefault(proj, []).append(t)

print('  Thread Routing (dry-run):')
print('  ─────────────────────────────')
for proj_path, proj_threads in project_threads.items():
    encoded = proj_path.replace('/', '-').replace('_', '-').lstrip('-')
    candidate = os.path.join(claude_projects_dir, f'-{encoded}', 'memory', 'MEMORY.md')
    exists = os.path.exists(candidate)
    if not exists:
        encoded2 = proj_path.replace('/', '-').lstrip('-')
        candidate = os.path.join(claude_projects_dir, f'-{encoded2}', 'memory', 'MEMORY.md')
        exists = os.path.exists(candidate)
    status = f'{len(proj_threads)} thread(s) would be written' if exists else 'skipped — no MEMORY.md'
    print(f'  {proj_path} ({status})')
    for t in proj_threads:
        print(f\"    {t['thread']} ({t.get('sessions', '?')} sessions)\")
" 2>/dev/null | while IFS= read -r line; do log "$line"; done
    log ""
fi

# ─── Update State ────────────────────────────────────────────────────────────
EFF_VERDICT_SAFE="${EFF_VERDICT:-insufficient_data}"
cat > "$LAST_RUN" << EOF
{
    "timestamp": "$(date -u '+%Y-%m-%dT%H:%M:%SZ')",
    "facets_processed": $TOTAL_FACETS,
    "sessions_analyzed": $SESSIONS,
    "friction_rate": "$FRICTION_RATE",
    "trend": "$TREND",
    "updates_applied": $UPDATE_COUNT,
    "effectiveness_verdict": "$EFF_VERDICT_SAFE"
}
EOF

log ""
log "═══════════════════════════════════════════════════"
log "  SELF-IMPROVEMENT RUN COMPLETE"
log "  Sessions: $SESSIONS | Friction: $FRICTION_RATE | Trend: $TREND"
if [ "$HAS_BASELINE" = "true" ]; then
    log "  Effectiveness: $EFF_VERDICT (friction delta: $EFF_FR_DELTA)"
fi
log "  Memory updates: $UPDATE_COUNT applied"
log "═══════════════════════════════════════════════════"

# macOS notification (silently skip on Linux)
NOTIF_BODY="Sessions: $SESSIONS | Friction: $FRICTION_RATE | Trend: $TREND"
if [ "$HAS_BASELINE" = "true" ]; then
    NOTIF_BODY="$NOTIF_BODY | Verdict: $EFF_VERDICT"
fi
osascript -e "display notification \"$NOTIF_BODY\" with title \"Claude Self-Improve\" subtitle \"$UPDATE_COUNT memory updates applied\"" 2>/dev/null

exit 0
